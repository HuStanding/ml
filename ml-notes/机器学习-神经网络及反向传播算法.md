

[toc]

# ä¸€ã€ç¥ç»ç½‘ç»œ

## 1.1 ç¥ç»ç½‘ç»œçš„éœ€æ±‚

çœ‹å®Œäº† Ng çš„ç¥ç»ç½‘ç»œè§†é¢‘ï¼Œè¿˜æ˜¯åŠæ‡‚ä¸æ‡‚ï¼Œæ‰€ä»¥è§‰å¾—è‡ªå·±æ¢³ç†ä¸€ä¸‹æ•´ä¸ªæµç¨‹ï¼Œæ–‡ä¸­çš„åå­—å®šä¹‰ä»¥åŠå…¬å¼é£æ ¼å‚è€ƒçš„ Ng çš„æ•™å­¦ææ–™ã€‚æˆ‘ä»¬å‰é¢è®²åˆ°ä½¿ç”¨ğŸ‘‰ [[logistic å›å½’]](https://zhuanlan.zhihu.com/p/95132284) é¢„æµ‹æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ª 0-1 åˆ†ç±»é—®é¢˜ï¼Œå¦‚æœæƒ³å®ç°å¤šåˆ†ç±»è¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚å¯¹ä¸‹é¢çš„æ‰‹å†™æ•°å­—è¿›è¡Œè¯†åˆ«ï¼Œè¾“å…¥çš„ç»´åº¦ >400 ï¼Œè¾“å‡ºçš„ç±»åˆ«æœ‰ 0-9 ä¸€å…± 10 ä¸ªç±»åˆ«ï¼Œä½ å½“ç„¶å¯ä»¥ä½¿ç”¨å¤šä¸ª logistic å›å½’æ¥å®ç°ï¼Œä¸€å…±è®­ç»ƒ 10 ä¸ª 0-1 åˆ†ç±»å™¨ï¼Œä½†æ˜¯è®­ç»ƒçš„ä»£ä»·æˆæœ¬éå¸¸å¤§å¤§ã€‚å½“æˆ‘ä»¬éœ€è¦è§£å†³ç‰¹å¾é‡å¾ˆå¤§çš„éçº¿æ€§åˆ†ç±»é—®é¢˜æ—¶ï¼ˆ æ¯”å¦‚è®¡ç®—æœºè§†è§‰é—®é¢˜ ï¼‰ï¼Œæˆ‘ä»¬åŸæœ¬å‡è®¾é«˜æ¬¡ç‰¹å¾çš„æ–¹æ³•ä¼šä½¿å¾—ç‰¹å¾æ•°å¼‚å¸¸åºå¤§ï¼Œä»è€Œå¼•å‡ºæ–°çš„æ–¹æ³•ç¥ç»ç½‘ç»œã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqn7admwoj30cy0cswfb.jpg" alt="image-20200105164117281" style="zoom:30%;" />

## 1.2 ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œ

ç¥ç»ç½‘ç»œ(Neural Network)æ˜¯ä¸€ç§å¾ˆå¤è€çš„ç®—æ³•ï¼Œä»–çš„æœ¬è´¨æ˜¯é€šè¿‡æ¨¡ä»¿äººç±»çš„ç¥ç»å…ƒä¿¡æ¯å¤„ç†æ–¹å¼æ¥å¤„ç†ç°å®ä¸­çš„æ•°æ®ã€‚è€ƒè™‘ä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»å…ƒæ¨¡å‹ï¼Œè¾“å…¥æ•°æ®ä¸º $x_1,x_2,x_3$ï¼ˆå›¾ä¸­çœç•¥äº†åç½®é¡¹ $x_0$ï¼‰ï¼Œç»è¿‡ä¸€ä¸ªç¥ç»å…ƒä¹‹åè¾“å‡ºå€¼ $h_{\theta}(x)$ ã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqnrpiozbj30xc07xwf7.jpg" alt="006tNbRwly1gaqms5ea02j30xc07x0xe" style="zoom:67%;" />

ç¥ç»å…ƒçš„å¤„ç†å¯ä»¥ä½¿ç”¨ `sigmoid` å‡½æ•°è¡¨ç¤ºå¦‚ä¸‹
$$
h_{\theta}(x)=\frac{1}{1+e^{-\theta^{T} x}} \quad x=\left[\begin{array}{l}{x_{0}} \\ {x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right] \quad \theta=\left[\begin{array}{l}{\theta_{0}} \\ {\theta_{1}} \\ {\theta_{2}} \\ {\theta_{3}}\end{array}\right]\tag{1}
$$
å•ä¸ªç¥ç»å…ƒçš„èƒ½åŠ›éå¸¸æœ‰é™ï¼Œè€Œç¥ç»ç½‘ç»œå¼ºå¤§çš„åœ°æ–¹åœ¨äºå°†è¿™äº›ç¥ç»å…ƒè¿æ¥åœ¨ä¸€èµ·å…±åŒå·¥ä½œï¼ˆ ç±»ä¼¼äºå¤§è„‘ä¸­ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç¥ç»ç½‘ç»œæ¨¡å‹æ˜¯å¦‚ä½•è¡¨ç¤ºçš„ã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqmsxxmq5j30xc0a8wie.jpg" alt="ä¼ä¸šå¾®ä¿¡æˆªå›¾_a4bacb9b-8c95-412d-a7b0-b62674e05254" style="zoom:50%;" />

ä¸Šå›¾æ˜¯å«æœ‰ä¸€å±‚éšå«å±‚çš„ç¥ç»ç½‘ç»œï¼Œè¾“å…¥å•å…ƒ $x_1,x_2,x_3$ å°†å€¼ä¼ ç»™éšå«å±‚ï¼ˆ æ¯ä¸ªè¾“å…¥å•å…ƒä¼ å…¥çš„æƒå€¼æ˜¯ä¸åŒçš„ ï¼‰ã€‚ç„¶åéšå«å±‚å°†è¾“å‡ºå€¼å†ä¼ ç»™è¾“å‡ºå±‚çš„ç¥ç»å…ƒã€‚ç”¨æ•°å­¦è¯­è¨€è¡¨è¾¾å¦‚ä¸‹
$$
\left[\begin{array}{l}{a_{1}^{(2)}} \\ {a_{2}^{(2)}} \\ {a_{3}^{(2)}}\end{array}\right]=g\left(\left[\begin{array}{c}{z_{1}^{(2)}} \\ {z_{2}^{(2)}} \\ {z_{3}^{(2)}}\end{array}\right]\right)=g\left(\left[\begin{array}{}{\Theta_{10}^{(1)}} & {\Theta_{11}^{(1)}} & {\Theta_{12}^{(1)}} & {\Theta_{13}^{(1)}} \\ {\Theta_{20}^{(1)}} & {\Theta_{21}^{(1)}} & {\Theta_{22}^{(1)}} & {\Theta_{23}^{(1)}} \\ {\Theta_{30}^{(1)}} & {\Theta_{31}^{(1)}} & {\Theta_{32}^{(1)}} & {\Theta_{33}^{(1)}}\end{array}\right]\left[\begin{array}{c}{x_{0}} \\ {x_{1}} \\ {x_{2}} \\ {x_{3}}\end{array}\right]\right)
$$
å…¶ä¸­ï¼Œ$z_i$ è¡¨ç¤ºä¸­é—´å˜é‡ï¼Œ$\Theta^{(l)}$ è¡¨ç¤ºç¬¬ $l$ å±‚çš„æƒé‡å‚æ•°ï¼Œ$a_i$ è¡¨ç¤ºç¥ç»å…ƒçš„è¾“å‡ºï¼Œ è¾“å…¥æ•°æ®åŠ å…¥äº†åç½®é¡¹ $x_0$ ï¼Œæœ€ç»ˆè¾“å‡ºçš„  $h_{\theta}(x)$ ä¸º
$$
h_{\Theta}(x)=a_{1}^{(3)}=g\left(z_{1}^{(3)}\right)=g\left(\Theta_{10}^{(2)} a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)} a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)\tag{2}
$$
è¿™é‡ŒåŒæ ·åŠ å…¥äº†ä¸€ä¸ªåç½®é¡¹ $a_{0}^{(2)}$ ï¼Œè¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªå•å…ƒï¼Œä½†å…¶å®å¯ä»¥åŒ…å«å¤šä¸ªå•å…ƒç”¨äºå¤šåˆ†ç±»ã€‚

åœ¨ä¸Šå›¾ä¸­ï¼Œå¦‚æœç¬¬ $j$ å±‚æœ‰ $s_j$ ä¸ªå•å…ƒï¼Œç¬¬ $j+1$ å±‚æœ‰ $s_{j+1}$ ä¸ªå•å…ƒï¼Œé‚£ä¹ˆå®¹æ˜“å¾—å‡º $\Theta^{(j)}$ ä¸º $s_{j+1}Ã—(s_j+1)$ çŸ©é˜µã€‚

# äºŒã€åå‘ä¼ æ’­

Ng åœ¨è§†é¢‘ä¸­è¯´å³ä½¿è¿™ä¸ªç®—æ³•å‡ºç°äº†å¾ˆå¤šå¹´åŒæ—¶æ•ˆæœä¹Ÿéå¸¸å¥½ï¼Œä½†æ˜¯è¦ç†è§£é‡Œé¢çš„åŸç†è¿˜æ˜¯å¾ˆéš¾ï¼ˆè°¦è™šçš„è¯´æ³• ğŸ˜•ï¼Œæˆ‘å°±æ˜¯ä¸æ‡‚ï¼‰ã€‚

## 2.1 ä»£ä»·å‡½æ•°

å‡è®¾æˆ‘ä»¬çš„å¤šåˆ†ç±»é—®é¢˜æœ‰ $K$ ä¸ªåˆ†ç±»ï¼Œç¥ç»ç½‘ç»œå…±æœ‰ $L$ å±‚ï¼Œæ¯ä¸€å±‚çš„ç¥ç»å…ƒä¸ªæ•°ä¸º $s_l$ ï¼Œè¾“å…¥æ•°æ®ä¸º $m$ ä¸ªï¼Œé‚£ä¹ˆç¥ç»ç½‘ç»œçš„ä»£ä»·å‡½æ•°ä¸º
$$
\begin{aligned} J(\Theta) =&-\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left(y_{k}^{(i)} \log \left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}+\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}\right)\right) \\
&+\frac{\lambda}{2 m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_{l}} \sum_{j=1}^{s_{l+1}}\left(\Theta_{j i}^{(l)}\right)^{2} \end{aligned}\tag{3}
$$
å…¶ä¸­ç¬¬ 2 é¡¹ä¸ºæ­£åˆ™åŒ–ç³»æ•°ã€‚

## 2.2 æ­£å‘ä¼ æ’­ç®—æ³•

è¦ç†è§£ä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨åå‘ä¼ æ’­(Backpropagation algorithm)ï¼Œé¦–å…ˆå°±å¾—äº†è§£ä¸€ä¸‹æ­£å‘ä¼ æ’­ä¸­ä½¿ç”¨çš„æ¢¯åº¦è®¡ç®—æ–¹æ³•ï¼Œå‡è®¾ç¥ç»ç½‘ç»œæ¨¡å‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸€å…±åŒ…å« 4 å±‚ï¼Œè®­ç»ƒæ•°æ®ä¸º $(x,y)$ã€‚

![image-20200105174200200](https://tva1.sinaimg.cn/large/006tNbRwly1galtqvyi60j30c207gwhn.jpg)

æ­£å‘ä¼ æ’­çš„è¡¨è¾¾å¼ä¸º
$$
\begin{aligned} a^{(1)} &=x \\ z^{(2)} &=\Theta^{(1)} a^{(1)} \\ a^{(2)} &=g\left(z^{(2)}\right)\left(\operatorname{add} a_{0}^{(2)}\right) \\ z^{(3)} &=\Theta^{(2)} a^{(2)} \\ a^{(3)} &=g\left(z^{(3)}\right)\left(\operatorname{add} a_{0}^{(3)}\right) \\ z^{(4)} &=\Theta^{(3)} a^{(3)} \\ a^{(4)} &=h_{\Theta}(x)=g\left(z^{(4)}\right) \end{aligned}
$$
ä¸ºäº†èƒ½å¤Ÿä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•æ¥è®­ç»ƒç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä»£ä»·å‡½æ•°çš„æ¢¯åº¦ã€‚ä¸€ç§å¾ˆç›´è§‚çš„æ–¹æ³•å°±æ˜¯ä½¿ç”¨æ•°å€¼è®¡ç®—ï¼Œå¯¹äºæŸä¸ª $\Theta_{ij}$ ï¼Œç»™å®ƒåŠ ä¸Šå‡å»ä¸€ä¸ªå¾ˆå°çš„é‡æ¥ $\epsilon$ è®¡ç®—æ¢¯åº¦ï¼Œå…¬å¼ä¸º
$$
\frac{\partial J(\theta)}{\partial \theta_{j}} \approx \frac{J\left(\theta_{1}, \cdots, \theta_{j}+\epsilon, \cdots, \theta_{n}\right)-J\left(\theta_{1}, \cdots, \theta_{j}-\epsilon, \cdots, \theta_{n}\right)}{2 \epsilon}\tag{4}
$$
ä½†ç¨å¾®åˆ†æä¸€ä¸‹ç®—æ³•çš„å¤æ‚åº¦å°±èƒ½çŸ¥é“ï¼Œè¿™æ ·çš„æ–¹æ³•ååˆ†ç¼“æ…¢ã€‚å¯¹äºæ¯ä¸€ç»„æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ‰€æœ‰æƒå€¼çš„æ¢¯åº¦ï¼Œ<font color=red>**æ€»çš„è®¡ç®—æ¬¡æ•° = è®­ç»ƒæ•°æ®ä¸ªæ•° x ç½‘ç»œæƒå€¼ä¸ªæ•° x å‰å‘ä¼ æ’­è®¡ç®—æ¬¡æ•°** </font>ã€‚åœ¨é€šå¸¸æƒ…å†µä¸‹è¿™æ ·çš„å¤æ‚åº¦æ˜¯æ— æ³•æ¥å—çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»…ä½¿ç”¨è¿™ä¸ªæ–¹æ³•æ¥éªŒè¯åå‘ä¼ æ’­ç®—æ³•è®¡ç®—çš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®ã€‚

## 2.3 åå‘ä¼ æ’­ç®—æ³•

ä¸ºäº†ä»‹ç»åå‘ä¼ æ’­ç®—æ³•ï¼Œæˆ‘ä»¬å…ˆå¤ä¹ ä¸€ä¸‹å¾®ç§¯åˆ†ä¸­çš„æ±‚å¯¼çš„é“¾å¼æ³•åˆ™ğŸ‘€ï¼Œå¯¹äºå¤šå…ƒå‡½æ•° $z=f(u,v)$ ï¼Œå…¶ä¸­ $u=h(x,y), v=g(x,y)$ï¼Œé‚£ä¹ˆ
$$
\begin{aligned}{\frac{\partial z}{\partial x}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial x}} \\ {\frac{\partial z}{\partial y}=\frac{\partial z}{\partial u} \frac{\partial u}{\partial y}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial y}}\end{aligned}
$$
ä¸€èˆ¬çš„ï¼Œå¯¹äºå‡½æ•° $y$ï¼Œ å¦‚æœä»–èƒ½çœ‹åš $z_1,z_2,\ldots,z_n$ çš„å‡½æ•°ï¼Œè€Œ $z_i$ ä¸º $t$ çš„å‡½æ•°ï¼Œé‚£ä¹ˆ $y$ å…³äº $t$ çš„æ¢¯åº¦ä¸º
$$
\frac{\partial y}{\partial t}=\sum_{i=1}^{n} \frac{\partial y}{\partial z_{i}} \frac{\partial z_{i}}{\partial t}\tag{5}
$$
Ng çš„è§†é¢‘ä¸­å…³äºè¿™ä¸€å—çš„æ¨å¯¼ç›´æ¥è·³è¿‡äº†ğŸ˜‚ï¼Œä¸‹é¢ç»™å‡ºåå‘ä¼ æ’­å…·ä½“çš„å…¬å¼æ¨å¯¼ï¼Œå‚è€ƒ [2] ä¸­çš„ä½œè€…ç»™å‡ºçš„è¯¦ç»†çš„è¯æ˜è¿‡ç¨‹ã€‚

å®šä¹‰ $\delta_{j}^{(l)}$ ä¸ºç¬¬ $l$ å±‚çš„ç¬¬ $j$ ä¸ªç¥ç»å…ƒçš„è¯¯å·®ï¼Œå…¶è¡¨è¾¾å¼ä¸º
$$
\delta_{j}^{(l)}=\frac{\partial J(\Theta)}{\partial z_{j}^{(l)}}\tag{6}
$$
è¾“å‡ºå±‚çš„è¯¯å·®ä¸º
$$
\delta_{j}^{(L)}=\frac{\partial J(\Theta)}{\partial z_{j}^{(L)}}=a_{j}^{L}-y_{j}^{L}\tag{7}
$$
å…¶å®ƒå±‚çš„è¯¯å·®ä¸º
$$
\delta_{j}^{(l)}=\sum_{k=1}^{s_{l+1}} \frac{\partial J(\Theta)}{\partial z_{k}^{(l+1)}} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\sum_{k=1}^{s_{l+1}} \delta_{k}^{(l+1)} \frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}
$$
å…¶ä¸­
$$
z_{k}^{(l+1)}=\sum_{p=1}^{s_{l}} \Theta_{k p}^{(l)} g\left(z_{p}^{(l)}\right)\tag{8}
$$
å½“ $p=j$ æ—¶ï¼Œä¸Šå¼å¯¹ $\partial z_{j}^{(l)}$ çš„åå¯¼æ•°æ‰ä¸ä¸º 0ï¼Œæ‰€ä»¥
$$
\frac{\partial z_{k}^{(l+1)}}{\partial z_{j}^{(l)}}=\Theta_{kj}^{(l)} g'\left(z_{j}^{(l)}\right)
$$
æ³¨æ„åˆ°ï¼Œ $g(z)=\frac{1}{1+e^{-z}}$ï¼Œæ‰€ä»¥
$$
g'(z)=\frac{e^{-z}}{(1+e^{-z})^2}=g(z)\left(1-g(z)\right)
$$
æ‰€ä»¥
$$
\delta_{j}^{(l)}=\sum_{k=1}^{s_{l+1}} \Theta_{k j}^{(l)} \delta_{k}^{(l+1)} g^{\prime}\left(z_{j}^{(l)}\right)=\sum_{k=1}^{s_{l+1}} \Theta_{k j}^{(l)} \delta_{k}^{(l+1)} a_{j}^{(l)}\left(1-a_{j}^{(l)}\right)\tag{9}
$$
æŒ‰ç…§ä¸Šè¿°æ€è·¯æˆ‘ä»¬å¯¹æ¯ä¸ªå‚æ•° $\Theta_{ij}^{(l)}$ æ±‚å¯¼ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™ä¸º
$$
\frac{\partial J(\Theta)}{\partial \Theta_{i j}^{(l)}}=\sum_{k=1}^{s_{l+1}} \frac{\partial J(\Theta)}{\partial z_{k}^{(l+1)}} \frac{\partial z_{k}^{(l+1)}}{\partial \Theta_{i j}^{(l)}}=\sum_{k=1}^{s_{l+1}} \delta_{k}^{(l+1)} \frac{\partial z_{k}^{(l+1)}}{\partial \Theta_{i j}^{(l)}}\tag{10}
$$
æ ¹æ®å¼ $(8)$ ï¼Œå¯ä»¥å¾—åˆ°
$$
\frac{\partial J(\Theta)}{\partial \Theta_{i j}^{(l)}}=\delta_{i}^{(l+1)}g\left(z_{j}^{(l)}\right)=\delta_{i}^{(l+1)}a_{j}^{(l)}\tag{11}
$$
æ ¹æ®å¼ $(7)ã€(9)ã€(11)$å¯ä»¥selå¾—åˆ°åå‘ä¼ æ’­ç®—æ³•çš„æ­¥éª¤ï¼š

> è¾“å…¥æ•°æ® $\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\ldots,(x^{(m)},y^{(m)})\}$
>
> 1. å¯¹æ‰€æœ‰çš„ $l,i,j$ ï¼Œåˆå§‹åŒ– $\Delta_{ij}^{(l)}=0$ï¼Œ åˆå§‹åŒ–æƒå€¼ $\Theta_{i j}^{(l)},-\delta \leq \Theta_{i j}^{(l)} \leq \delta$
>
> 2. for $i=1 \to m$
>
>     1. ä»¤ $a^{(1)}=x^{(i)}$
>     2. å‰å‘ä¼ æ’­è®¡ç®— $a^{(l)},l=2,3,\ldots,L$
>     3. ä½¿ç”¨ $(7)$ å¼è®¡ç®— $\delta^{(L)}=a^{L}-y^{L}$
>     4. ä½¿ç”¨ $(9)$ å¼è®¡ç®— $\delta^{(L-1)},\delta^{(L-2)},\ldots,\delta^{(2)}$
>     5. ä½¿ç”¨ $(11)$ å¼è®¡ç®— $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+\delta_{i}^{(l+1)}a_{j}^{(l)}$
>
> 3. è·å–æ¢¯åº¦çŸ©é˜µ(ä¸å¯¹ç¬¬ä¸€é¡¹åŠ æ­£åˆ™åŒ–é¡¹)
>     $$
>     D_{i j}^{(l)}=\left\{\begin{array}{ll}{\frac{1}{m} \Delta_{i j}^{(l)}+\lambda \Theta_{i j}^{(l)}} & {\text { if } j \neq 0} \\ {\frac{1}{m} \Delta_{i j}^{(l)}} & {\text { if } j=0}\end{array}\right.
>     $$
>
> 4. æ›´æ–°æƒå€¼ $\Theta^{(l)}:=\Theta^{(l)}-\alpha D^{(l)}$

è¿™é‡Œè¯´ä¸€ä¸‹**æƒå€¼åˆå§‹åŒ–**ï¼Œå¯¹äºç¥ç»ç½‘ç»œï¼Œä¸èƒ½åƒä¹‹å‰é‚£æ ·ä½¿ç”¨ç›¸åŒçš„ 0 å€¼æ¥åˆå§‹åŒ–ï¼Œè¿™ä¼šå¯¼è‡´æ¯å±‚çš„é€»è¾‘å•å…ƒéƒ½ç›¸åŒã€‚å› æ­¤æˆ‘ä»¬ä½¿ç”¨éšæœºåŒ–çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œä½¿å¾— $-\delta \leq \Theta_{i j}^{(l)} \leq \delta$ ã€‚

# ä¸‰ã€ä»£ç å®ç°

## 3.1 Python æ‰‹åŠ¨å®ç°

æˆ‘ä»¬ä½¿ç”¨ ğŸ‘‰ [[softmaxå›å½’]](https://zhuanlan.zhihu.com/p/98061179) ä¸­ç±»ä¼¼çš„æ•°æ®ï¼Œä½†æ˜¯æ•°æ®é‡å°‘ä¸€äº›ï¼Œä¸»è¦æ˜¯ä¸ºäº†åŠ å¿«è®­ç»ƒçš„é€Ÿåº¦ï¼Œå˜¿å˜¿ğŸ˜€ã€‚

æ•°æ®é›†ä¸€å…±åˆ†ä¸º 4 ä¸ªç±»åˆ«ï¼Œåˆ†å¸ƒæƒ…å†µå¦‚ä¸‹

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqll9psmjj30xc0m8tao.jpg" alt="æ•°æ®é›†åˆ†å¸ƒ" style="zoom:50%;" />

å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®ï¼Œè®­ç»ƒæ•°æ®é›†çš„å¤§å°ä¸º 150 * 2ï¼Œæµ‹è¯•æ•°æ®é›†çš„å¤§å°ä¸º 50 * 2ã€‚

é¦–å…ˆå®šä¹‰å‡ ä¸ªå·¥å…·å‡½æ•°ï¼š

```python
def load_dataset(file_path):
    dataMat = []
    labelMat = []
    fr = open(file_path)
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return np.array(dataMat), np.array(labelMat).reshape((-1,1))


def sigmoid(x):
    return 1.0 / (1 + np.exp(-x))


def one_hot(label_arr, n_samples, n_classes):
    one_hot = np.zeros((n_samples, n_classes))
    one_hot[np.arange(n_samples), label_arr.T] = 1
    return one_hot
  
def plot_loss(loss):
    fig = plt.figure(figsize=(8,5))
    plt.plot(np.arange(len(all_loss)), all_loss)
    plt.title("Development of loss during training")
    plt.xlabel("Number of iterations")
    plt.ylabel("Loss")
    plt.show()
```

æ¥ä¸‹æ¥æ˜¯ç¥ç»ç½‘ç»œçš„ä¸»ä½“éƒ¨åˆ†

```python
class NeuralNetWork():
    def __init__(self, epsilon = 1, iters = 1000, alpha = 0.01, lam = 0.01):
        self.hidden_dim = 10    # é»˜è®¤å– 4 ä¸ªéšè—å±‚ç¥ç»å…ƒ
        self.n_hidden = 1       # é»˜è®¤å– 1 ä¸ªéšè—å±‚
        self.epsilon = epsilon  # éšæœºåˆå§‹åŒ–æƒå€¼çŸ©é˜µçš„ç•Œé™
        self.iters = iters      # æœ€å¤§è¿­ä»£æ¬¡æ•°
        self.alpha = alpha      # å­¦ä¹ ç‡
        self.lam = lam          # æ­£åˆ™åŒ–é¡¹ç³»æ•°
        self.weights = list()   # åˆå§‹åŒ–æƒå€¼çŸ©é˜µ
        self.gradients = list() # åˆå§‹åŒ–æ¢¯åº¦çŸ©é˜µ
        self.bias = 1           # åç½®é¡¹


    def init_weights(self, n_input, n_output):
        # ç¬¬ 1 â†’ 2 å±‚çš„æƒå€¼çŸ©é˜µ
        self.weights.append((2 * self.epsilon) * np.random.rand(self.hidden_dim, n_input + 1) - self.epsilon)
        # ç¬¬ 2 â†’ n å±‚çš„æƒå€¼çŸ©é˜µ
        for i in range(self.n_hidden - 1):
            self.weights.append((2 * self.epsilon) * np.random.rand(self.hidden_dim, self.hidden_dim + 1) - self.epsilon)
        self.weights.append((2 * self.epsilon) * np.random.rand(n_output, self.hidden_dim + 1) - self.epsilon)
    

    def init_gradients(self, n_input, n_output):
        # ç¬¬ 1 â†’ 2 å±‚çš„æ¢¯åº¦çŸ©é˜µ
        self.gradients.append(np.zeros((self.hidden_dim, n_input + 1)))
        # ç¬¬ 2 â†’ n å±‚çš„æ¢¯åº¦çŸ©é˜µ
        for i in range(self.n_hidden - 1):
            self.gradients.append(np.zeros((self.hidden_dim, self.hidden_dim + 1)))
        self.gradients.append(np.zeros((n_output, self.hidden_dim + 1)))


    def train(self, data_arr, label_arr):
        n_samples, n_features = data_arr.shape
        n_output = len(set(label_arr.flatten()))   # è¾“å‡ºç±»åˆ«ä¸ªæ•°
        y_one_hot = one_hot(label_arr, n_samples, n_output)
        all_loss = list()  # æŸå¤±å‡½æ•°è®°å½•
        self.init_weights(n_features, n_output)
        self.init_gradients(n_features, n_output)
        for it in range(self.iters):
            for index in range(n_samples):
                # è®¡ç®—å‰å‘ä¼ æ’­æ¯ä¸€å±‚çš„è¾“å‡ºå€¼
                layer_output = self.forward_propagation(data_arr[index])
                # è®¡ç®—æ¯ä¸€å±‚çš„è¯¯å·®
                layer_error = self.cal_layer_error(layer_output, y_one_hot[index])
                # è®¡ç®—æ¢¯åº¦çŸ©é˜µ
                self.cal_gradients(layer_output, layer_error)
            # æ›´æ–°æƒå€¼
            self.update_weights(n_samples)
            # ç´¯åŠ è¾“å‡ºè¯¯å·®
            #loss = self.cal_loss(data_arr, y_one_hot, n_samples)
            #all_loss.append(loss)
        return self.weights, all_loss

    
    def forward_propagation(self, data):
        layer_output = list()
        a = np.insert(data, 0, self.bias)
        layer_output.append(a)
        for i in range(self.n_hidden + 1):
            z = self.weights[i] @ a
            a = sigmoid(z)
            if i != self.n_hidden:
                a = np.insert(a, 0, self.bias)
            layer_output.append(a)
        return np.array(layer_output)
    

    def cal_layer_error(self, layer_output, y):
        # åªæœ‰ç¬¬ 2 â†’n å±‚æœ‰è¯¯å·®ï¼Œè¾“å…¥å±‚æ²¡æœ‰è¯¯å·®
        layer_error = list()
        # è®¡ç®—è¾“å‡ºå±‚çš„è¯¯å·®
        error = layer_output[-1] - y
        layer_error.append(error)
        # åå‘ä¼ æ’­è®¡ç®—è¯¯å·®
        for i in range(self.n_hidden, 0, -1):
            error = self.weights[i].T @ error * layer_output[i] * (1 - layer_output[i])
            # åˆ é™¤ç¬¬ä¸€é¡¹ï¼Œåç½®é¡¹æ²¡æœ‰è¯¯å·®
            error = np.delete(error, 0)
            layer_error.append(error)
        return np.array(layer_error[::-1])
    

    def cal_gradients(self, layer_output, layer_error):
        for l in range(self.n_hidden + 1):
            for i in range(self.gradients[l].shape[0]):
                for j in range(self.gradients[l].shape[1]):
                    self.gradients[l][i][j] += layer_error[l][i] * layer_output[l][j]


    def update_weights(self, n_samples):
        for l in range(self.n_hidden + 1):
            gradient = 1.0 / n_samples * self.gradients[l] + self.lam * self.weights[l]
            gradient[:,0] -= self.lam * self.weights[l][:,0]
            self.weights[l] -= self.alpha * gradient


    def cal_loss(self, data_arr, y_one_hot, n_samples):
        loss = 0  # è¿™é‡Œä¸ç”¨æ·»åŠ æ­£åˆ™åŒ–é¡¹
        for i in range(n_samples):
            y = y_one_hot[i]
            output = self.forward_propagation(data_arr[i])[-1]
            loss += np.sum((y * np.log(output) + (1 - y) * np.log(1 - output)))
        loss = (-1 / n_samples) * loss
        return loss
    

    def predict(self, data_arr):
        n_samples = data_arr.shape[0]
        ret = np.zeros(n_samples)
        for i in range(n_samples):
            output = self.forward_propagation(data_arr[i])[-1]
            ret[i] = np.argmax(output)
        return ret.reshape((-1,1))
    

    def plot_decision_boundary(self, X, y):
        # Set min and max values and give it some padding
        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
        h = 0.01
        # Generate a grid of points with distance h between them
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        # Predict the function value for the whole gid
        Z = self.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        # Plot the contour and training examples
        plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
        plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(y))
        plt.show()
```

ä¸»å‡½æ•°

```python
if __name__ == "__main__":
    # åŠ è½½æ•°æ®
    train_data_arr, train_label_arr = load_dataset('train_dataset.txt')
    test_data_arr, test_label_arr = load_dataset('test_dataset.txt')

    # è®­ç»ƒæ•°æ®
    nn = NeuralNetWork(iters = 1000)
    weights, all_loss = nn.train(train_data_arr, train_label_arr)
    #print(weights)
    y_predict = nn.predict(test_data_arr)
    accurcy = np.sum(y_predict == test_label_arr) / len(test_data_arr)
    print(accurcy)

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    #nn.plot_decision_boundary(test_data_arr, test_label_arr)
    # ç»˜åˆ¶æŸå¤±å‡½æ•°
    #plot_loss(all_loss)
```

è¾“å‡ºçš„å‡†ç¡®ç‡ä¸º

```
0.98
```

å¯ä»¥çœ‹åˆ°æµ‹è¯•çš„æ•ˆæœè¿˜æ˜¯å¾ˆä¸é”™çš„ï¼Œè®­ç»ƒçš„æ—¶å€™ä¿å­˜äº†æ¯ä¸€æ¬¡è¿­ä»£çš„æŸå¤±å€¼ï¼ŒæŸå¤±å‡½æ•°çš„å˜åŒ–æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqlqt0dhlj318g0rsabj.jpg" alt="æŸå¤±å‡½æ•°" style="zoom:40%;" />

ä¸ºäº†å½¢è±¡åŒ–çš„è§‚å¯Ÿåˆ†ç±»çš„æƒ…å†µï¼Œå‡½æ•°ä¸­æ·»åŠ äº†ä¸€ä¸ªç»˜åˆ¶å†³ç­–è¾¹ç•Œçš„å‡½æ•° `plot_decision_boundary` ï¼Œå…¶åˆ†å¸ƒæƒ…å†µå¦‚ä¸‹

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqluh1a6wj30zk0qo3zh.jpg" alt="å†³ç­–è¾¹ç•Œ" style="zoom:40%;" />

## 3.2 sklearn åº“å‡½æ•°å®ç°

`sklearn` åº“ä¸­çš„åŒ… `MLPClassifier` ï¼Œå°è£…äº†ç¥ç»ç½‘ç»œçš„ç›¸å…³åŠŸèƒ½ï¼Œæˆ‘ä»¬ç”¨ä¸ŠèŠ‚ç›¸åŒçš„æ•°æ®ï¼Œå¯¹æ¯”è§‚å¯Ÿåº“å‡½æ•°ç”Ÿæˆçš„ç»“æœã€‚

é¦–å…ˆæ˜¯å¯¼å…¥æ•°æ®ï¼Œç¥ç»ç½‘ç»œå¯¹æ•°æ®å°ºåº¦æ•æ„Ÿï¼Œéœ€è¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–çš„è½¬æ¢ã€‚

```python
train_data_arr, train_label_arr = load_dataset('train_dataset.txt')
test_data_arr, test_label_arr = load_dataset('test_dataset.txt')

scaler = StandardScaler() # æ ‡å‡†åŒ–è½¬æ¢
scaler.fit(train_data_arr)  # è®­ç»ƒæ ‡å‡†åŒ–å¯¹è±¡
train_data_arr = scaler.transform(train_data_arr)   # è½¬æ¢æ•°æ®é›†
scaler.fit(test_data_arr)  # è®­ç»ƒæ ‡å‡†åŒ–å¯¹è±¡
test_data_arr = scaler.transform(test_data_arr)   # è½¬æ¢æ•°æ®é›†
```

ç„¶åæ˜¯è®­ç»ƒæ•°æ®ï¼Œè¾“å‡ºé¢„æµ‹çš„å‡†ç¡®ç‡

```python
from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5,), random_state=1)
clf.fit(train_data_arr, train_label_arr)
print('æ¯å±‚ç½‘ç»œå±‚ç³»æ•°çŸ©é˜µç»´åº¦ï¼š\n',[coef.shape for coef in clf.coefs_])
cengindex = 0
for wi in clf.coefs_:
cengindex += 1  # è¡¨ç¤ºåº•ç¬¬å‡ å±‚ç¥ç»ç½‘ç»œã€‚
print('ç¬¬%då±‚ç½‘ç»œå±‚:' % cengindex)
print('æƒé‡çŸ©é˜µç»´åº¦:',wi.shape)
print('ç³»æ•°çŸ©é˜µ:\n',wi)

r = clf.score(train_data_arr, train_label_arr)
print("Rå€¼(å‡†ç¡®ç‡):", r)  # 1.0

y_predict = clf.predict(test_data_arr).reshape((-1,1))
accurcy = np.sum(y_predict == test_label_arr) / len(test_data_arr)
print(accurcy)    # 0.98
```

ç»˜åˆ¶å†³ç­–è¾¹ç•Œ

```python
X = test_data_arr
y = test_label_arr
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
h = 0.01
# Generate a grid of points with distance h between them
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
# Predict the function value for the whole gid
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# Plot the contour and training examples
plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(y))
plt.show()
```

æˆ‘ä»¬çœ‹åˆ°è®­ç»ƒçš„å‡†ç¡®ç‡ä¸º `0.98` ï¼Œå’Œæˆ‘ä»¬ä¸Šä¸€èŠ‚çš„æ‰‹åŠ¨ Python è¾“å‡ºçš„ç»“æœç›¸åŒï¼Œè€Œå†³ç­–è¾¹ç•Œç›¸æ¯”æ‰‹åŠ¨ Python ä»£ç è¾“å‡ºçš„æ•ˆæœè¦å¥½å¾ˆå¤šï¼Œè¯´æ˜å’±ä»¬çš„ä»£ç è¿˜æ˜¯æœ‰ä¼˜åŒ–çš„ç©ºé—´çš„ã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaqm5j5vkoj30zk0qodgt.jpg" alt="ä½¿ç”¨sklearnäº§ç”Ÿçš„å†³ç­–è¾¹ç•Œ" style="zoom:40%;" />

## 3.3 å¯ä»¥ä¼˜åŒ–çš„ç‚¹

+ æ¿€æ´»å‡½æ•°å¯ä»¥æ›´æ¢æˆ `tanh`, `RELU`ç­‰
+ è®¾ç½®ä¸åŒçš„éšè—å±‚ç»´åº¦å’Œä¸ªæ•°
+ æ”¹å˜æ¢¯åº¦ä¸‹é™ç­–ç•¥
+ â€¦â€¦



<font color=red>æœ¬æ–‡çš„å®Œæ•´ä»£ç å’Œæ•°æ®å¯ä»¥å»ğŸ‘‰[[æˆ‘çš„ github\]](https://github.com/HuStanding/nlp-exercise/tree/master/machine_learning/neural_network)æŸ¥çœ‹</font>

# å››ã€å‚è€ƒ

[1] Andrew Ng. Machine Learning

[2] https://zhuanlan.zhihu.com/p/58068618

[3] https://zhuanlan.zhihu.com/p/57760693