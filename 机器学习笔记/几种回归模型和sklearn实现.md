

[toc]

# 1. ä»€ä¹ˆæ˜¯å›å½’ï¼Ÿ

åˆ†ç±»çš„ç›®æ ‡å˜é‡æ˜¯æ ‡ç§°å‹æ•°æ®ï¼Œè€Œå›å½’æ˜¯å¯¹è¿ç»­å‹æ•°æ®çš„é¢„æµ‹ã€‚å›å½’åˆ†ææ˜¯ä¸€ç§é¢„æµ‹å»ºæ¨¡æŠ€æœ¯ï¼Œç ”ç©¶å› å˜é‡å’Œè‡ªå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå¦‚é”€å”®é‡é¢„æµ‹æˆ–åˆ¶é€ ç¼ºé™·é¢„æµ‹ç­‰ï¼Œä¸‹å›¾ä¸­çš„çº¢çº¿è¡¨ç¤ºçš„å°±æ˜¯å›å½’æ›²çº¿ã€‚

![å›å½’åˆ†æ](https://tva1.sinaimg.cn/large/006y8mN6ly1g9ds4ubhbjj30j907zweh.jpg)

å›å½’ä¸åŒäºåˆ†ç±»å’Œèšç±»ï¼Œä»–ä»¬çš„åŒºåˆ«å¯ä»¥ç”¨ä¸‹å›¾å½¢è±¡çš„è¡¨è¾¾å‡ºæ¥ã€‚

![å›å½’-åˆ†ç±»-èšç±»çš„åŒºåˆ«](https://tva1.sinaimg.cn/large/006y8mN6ly1g9ds7wqc8rj30r20c6djc.jpg)

# 2. å›å½’æ¨¡å‹

è¿™é‡Œä½¿ç”¨`sklearn`è¿›è¡Œä»£ç å®ç°ï¼Œå¦‚æœæƒ³æ‰‹åŠ¨å®ç°çš„è¯ï¼Œå¯ä»¥çœ‹ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹ï¼Œé‚£æœ¬ä¹¦æœ‰éƒ¨åˆ†çš„ç®—æ³•å®ç°ï¼Œä¸‹é¢ä»‹ç»çš„ç®—æ³•ç»Ÿä¸€ä½¿ç”¨çš„å‡½æ•°å¦‚ä¸‹

+ **åŠ è½½æ•°æ®** æ•°æ®æ–‡ä»¶[ç‚¹è¿™é‡Œ](https://github.com/HuStanding/nlp-exercise/blob/master/regression/ex0.txt)

```python
def load_data(file_path):
    num_feat = len(open(file_path).readline().split("\t")) - 1
    data_mat = list()
    lable_mat = list()
    fr = open(file_path)
    for line in fr.readlines():
        line_arr = list()
        cur_line = line.strip().split("\t")
        for i in range(num_feat):
            line_arr.append(float(cur_line[i]))
        data_mat.append(line_arr)
        lable_mat.append(float(cur_line[-1]))
    return data_mat, lable_mat
```

+ **ç»˜åˆ¶å›å½’ç»“æœ**

```python
def plot_regression(model, x_data, y_data):
    x_data = np.mat(x_data)
    y_data = np.mat(y_data).T
    x_train, y_train = x_data[:150,1:], y_data[:150,:]
    x_test, y_test = x_data[150:,1:], y_data[150:,:]
    model.fit(x_train, y_train)
    score = model.score(x_test, y_test)
    result = model.predict(x_train)
    plt.figure()
    srt_idx = x_train.argsort(0)
    plt.plot(x_train[srt_idx].reshape(-1,1), y_train[srt_idx].reshape(-1,1), 'go', label = "true value")
    plt.plot(x_train[srt_idx].reshape(-1,1), result[srt_idx].reshape(-1,1), 'ro-', label = "predict value")
    plt.title("score:%f" % score)
    plt.legend()
    plt.show()
```

## 2.1 çº¿æ€§å›å½’

### 2.1.1 æ™®é€šçº¿æ€§å›å½’

æåˆ°å›å½’ï¼Œé¦–å…ˆæƒ³åˆ°çš„è‚¯å®šæ˜¯**çº¿æ€§å›å½’(linear regression)**ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å®¹æ˜“ç†è§£ï¼Œæœ€ç®€å•çš„å›å½’æ–¹æ³•ã€‚è®¾å¾…æ‹Ÿåˆçš„æ•°æ®å¯¹è±¡ä¸º$X=\{x_1,x_2,...,x_m\}$ï¼Œå…¶å¯¹åº”çš„çœŸå®å€¼ä¸º$y=\{y_1,y_2,...,y_m\}$ï¼Œçº¿æ€§æ¨¡å‹å¯ä»¥å†™ä¸º
$$
\hat{y}=Xw
$$
å…¶ä¸­ï¼Œ$w$ä¸º**å›å½’ç³»æ•°**ï¼Œæˆ‘ä»¬ç”¨å¹³æ–¹è¯¯å·®æ¥è¡¡é‡æ‹Ÿåˆçš„è¯¯å·®
$$
L(X)=\sum_{i=1}^{m}\left ( y_i-x_{i}^{T}w\right )^2=\left ( y-Xw\right )^2
$$
ä¸Šå¼å¯¹$w$æ±‚å¯¼ç­‰äº0å¯ä»¥å¾—åˆ°
$$
\frac{\partial L(X)}{\partial w}=\frac{\partial (y^Ty-w^TX^Ty-y^TXw-w^TX^TXw)}{\partial w}=2X^T\left ( y-Xw\right )=0
$$
å¯ä»¥å¾—åˆ°
$$
\hat{w}=(X^TX)^{-1}X^Ty
$$
ä¸Šè¿°æ–¹å¼å®¹æ˜“å¯¹è®­ç»ƒæ•°æ®æ¬ æ‹Ÿåˆï¼Œä¸€ç§å¥½çš„è§£å†³æ–¹å¼æ˜¯**å±€éƒ¨åŠ æƒçº¿æ€§å›å½’**ï¼Œä¸ºæ¯ä¸ªè¯¯å·®å¢åŠ ä¸€ä¸ªæƒé‡$w_i$(è¿™é‡Œçš„$w$å¹¶ä¸æ˜¯ä¸Šé¢çš„$\hat{w}$)ï¼Œæ­¤æ—¶è¯¯å·®å‡½æ•°å¯ä»¥å†™æˆ
$$
L(X)=\sum_{i=1}^{m}w_i\left ( y_i-x_{i}^{T}w\right )^2=[W( y-Xw)]^2
$$
å…¶ä¸­ï¼Œ$W$æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œä¹Ÿå«åšæ ¸ï¼Œæ ¸çš„ç±»å‹å¯ä»¥è‡ªç”±é€‰æ‹©ï¼Œæœ€å¸¸è§çš„å°±æ˜¯é«˜æ–¯æ ¸ï¼Œé«˜æ–¯æ ¸å¯¹åº”çš„æƒé‡å¦‚ä¸‹
$$
W(j,j)=\exp\left(\frac{\|x^{i}-x^{j}\|^{2}}{-2k^2}\right)
$$
åŒæ ·çš„ï¼Œå¯¹æ–°çš„è¯¯å·®å‡½æ•°$L(X)$æ±‚å¯¼å¯ä»¥å¾—åˆ°æ­¤æ—¶å›å½’ç³»æ•°ä¸º
$$
\hat{w}=(X^TWX)^{-1}X^TWy
$$
è¿™é‡Œçš„$W$å…¶å®æ˜¯$W^TW$ï¼Œä½†æ˜¯ä½¿ç”¨$W$ä»£æ›¿å…·æœ‰åŒæ ·çš„æ„ä¹‰å¹¶ä¸”ç®€ä¾¿ã€‚

`sklearn`è°ƒç”¨ä»£ç ï¼š

```python
x_data, y_data = load_data("ex0.txt")
from sklearn import linear_model
# çº¿æ€§å›å½’
model_linear_regression = linear_model.LinearRegression()
plot_regression(model_linear_regression, x_data, y_data)
```

ç»˜åˆ¶å‡ºçš„å›å½’æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º

![linear regression](https://tva1.sinaimg.cn/large/006tNbRwly1g9fe3d6taij318g0m8mz2.jpg)

### 2.1.2 å²­å›å½’

æˆ‘ä»¬çœ‹çº¿æ€§å›å½’ä¸­çš„è¾“å…¥é›†$X=\{x_1,x_2,...,x_m\}$ï¼Œå‡è®¾å…¶ç»´åº¦ä¸º$n$ï¼Œå½“$n>m$çš„æ—¶å€™ï¼Œ$X$ä¸æ˜¯æ»¡ç§©çŸ©é˜µï¼Œæ— æ³•æ±‚è§£é€†çŸ©é˜µï¼Œè¿™æ—¶å€™å°±éœ€è¦ç”¨åˆ°**å²­å›å½’(ridge regression)**äº†ï¼Œåœ¨çŸ©é˜µ$X^TX$ä¸ŠåŠ ä¸Šä¸€ä¸ª$\lambda I$è®©å…¶æˆä¸ºæ»¡ç§©çŸ©é˜µï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™çš„å›å½’ç³»æ•°ä¸º
$$
\hat{w}=(X^TX+\lambda I)^{-1}X^Ty
$$

`sklearn`è°ƒç”¨ä»£ç ï¼š

```python
x_data, y_data = load_data("ex0.txt")
from sklearn import linear_model
# Ridgeå›å½’
model_ridge = linear_model.Ridge(alpha = 0.01)
plot_regression(model_ridge, x_data, y_data)
```

ç»˜åˆ¶å‡ºçš„å›å½’æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º

![ridge regression](https://tva1.sinaimg.cn/large/006tNbRwly1g9fe3oyiyvj318g0m8abz.jpg)

## 2.2 å†³ç­–æ ‘å›å½’

å†³ç­–æ ‘å­¦ä¹ å¸¸ç”¨çš„ç®—æ³•æœ‰**ID3ã€C4.5ã€CART(classification and regression tree)**ï¼Œè¿™ä»‹ç»ç”¨äºå›å½’çš„å†³ç­–æ ‘**CART**ï¼Œå…·ä½“çš„æ–¹æ³•ç†è®ºå‚è€ƒæèˆªçš„ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ã€‚

æˆ‘ä»¬è€ƒè™‘è¾“å…¥çš„è®­ç»ƒæ•°æ®$D=\{X,y\}=\{(x_1,y_1),(x_1,y_1),...,(x_m,y_m)\}$ï¼Œä¸€ä¸ªå›å½’æ ‘å¯¹åº”ç€è¾“å…¥ç©ºé—´ï¼ˆå³ç‰¹å¾ç©ºé—´ï¼‰çš„ä¸€ä¸ªåˆ’åˆ†ä»¥åŠåœ¨åˆ’åˆ†çš„å•å…ƒä¸Šçš„è¾“å‡ºå€¼ï¼Œå‡è®¾å·²å°†è¾“å…¥ç©ºé—´åˆ’åˆ†ä¸º$M$ä¸ªå•å…ƒ$R_1,R_2,...,R_M$ï¼Œå¹¶ä¸”åœ¨æ¯ä¸€ä¸ªå•å…ƒä¸Šéƒ½æœ‰ä¸€ä¸ªå›ºå®šçš„è¾“å‡ºå€¼$c_m$ï¼Œé‚£ä¹ˆå›å½’æ ‘æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º
$$
f(x)=\sum_{m=1}^{M}c_mI(x \in R_m)
$$
å…¶ä¸­ï¼Œå‡½æ•°$I$å¯¹åº”ç€0-1å‡½æ•°ã€‚å½“è¾“å…¥ç©ºé—´çš„åˆ’åˆ†ç¡®å®šæ—¶ï¼Œå¯ä»¥ç”¨å¹³æ–¹è¯¯å·®$\sum_{x_i \in R_m}(y_i-f(x_i))$æ¥è¡¨ç¤ºå›å½’æ ‘å¯¹äºè®­ç»ƒæ•°æ®çš„é¢„æµ‹è¯¯å·®ï¼Œç”¨å¹³æ–¹è¯¯å·®æœ€å°çš„å‡†åˆ™æ±‚è§£æ¯ä¸ªå•å…ƒä¸Šçš„æœ€ä¼˜è¾“å‡ºå€¼ï¼Œé‚£ä¹ˆå•å…ƒ$R_m$ä¸Šçš„æœ€ä¼˜å€¼$\hat {c_m}$æ˜¯$R_m$ä¸Šçš„æ‰€æœ‰è¾“å…¥å®ä¾‹$x_i$å¯¹åº”çš„è¾“å‡º$y_i$çš„å‡å€¼ï¼Œå³
$$
\hat {c_m} = ave(y_i|x_i \in R_m)
$$
ä¸Šé¢æ˜¯æ•´ä¸ªæ ‘çš„è¾“å‡ºå½¢å¼ï¼Œå…³é”®çš„é—®é¢˜æ¥äº†ï¼Œæ€ä¹ˆå¯¹è¾“å…¥ç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Ÿè¿™é‡Œé‡‡ç”¨å¯å‘å¼çš„ç®—æ³•ï¼Œé€‰æ‹©ç¬¬$j$ä¸ªå˜é‡å’Œå®ƒå–çš„å€¼$s$ä½œä¸ºåˆ‡åˆ†å˜é‡(spliting variable)å’Œåˆ‡åˆ†ç‚¹(spliting point)ï¼Œå¹¶å®šä¹‰ä¸¤ä¸ªåŒºåŸŸ
$$
R_1(j,s)=\{x|x_jâ‰¤s\} \quad R_2(j,s)=\{x|x_j>s\}
$$
ç„¶åå¯»æ‰¾æœ€ä¼˜åˆ‡åˆ†å˜é‡å’Œæœ€ä¼˜åˆ‡åˆ†ç‚¹ï¼Œå³
$$
m(s)=\min_{j,s}\left[\min_{c_1}\sum_{x_i \in R_j(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i \in R_j(j,s)}(y_i-c_2)^2\right]
$$
ç®€å•çš„ç†è§£ï¼Œå°±æ˜¯åœ¨è¦æ±‚åˆ‡åˆ†ç‚¹$s$ä¸¤è¾¹çš„åŒºåŸŸçš„å‡æ–¹å·®éƒ½å°½é‡å°çš„åŒæ—¶ï¼Œä¿è¯ä¸¤ä¸ªåŒºåŸŸçš„æœ€å°å‡æ–¹å·®å’Œæ˜¯æœ€å°çš„ã€‚

å¯¹æ¯ä¸€å¯¹$(j,s)$ï¼Œå‡å€¼è¡¨ç¤ºä¸º
$$
\hat {c_1} = ave(y_i|x_i \in R_1(j,s))\quad \hat {c_2} = ave(y_i|x_i \in R_2(j,s))
$$
éå†æ‰€æœ‰è¾“å…¥å˜é‡ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„å¯¹$(j,s)$ï¼Œä»è€Œå°†è¾“å…¥ç©ºé—´åˆ‡åˆ†ä¸ºä¸¤ä¸ªåŒºåŸŸï¼Œæ¥ç€å¯¹åˆ‡åˆ†çš„ä¸¤ä¸ªåŒºåŸŸé‡å¤ä¸Šè¿°åˆ’åˆ†è¿‡ç¨‹ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ä¸ºæ­¢ï¼Œè¿™æ ·ä¸€ä¸ªå›å½’æ ‘çš„ç”Ÿæˆå°±å®Œæˆäº†ã€‚

ä¸¾ä¸ªğŸŒ°ï¼Œè¾“å…¥æ•°æ®$D$å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

| $x_i$ | 1     | 2     | 3     | 4     | 5     | 6     |
| ---- | ---- | ----- | ----- | ----- | ----- | ----- |
| $y_i$ | 5\.56 | 5\.70 | 5\.91 | 6\.40 | 6\.90 | 7\.95 |

å¯¹ä¸Šè¿°è¿ç»­å‹å˜é‡ï¼Œåªæœ‰ä¸€ä¸ªåˆ‡åˆ†å˜é‡ï¼Œé‚£ä¹ˆè€ƒè™‘åˆ‡åˆ†ç‚¹ä¸º1.5, 2.5, 3.5, 4.5, 5.5ã€‚å¯¹åˆ‡åˆ†ç‚¹ä¾æ¬¡æ±‚è§£$R_1,R_2,c_1,c_2,m(s)$ï¼Œä¾‹å¦‚å½“åˆ‡åˆ†ç‚¹ä¸º2.5æ—¶ï¼Œ$R_1=\{1,2\},R_2=\{3,4,5,6\}$ï¼Œå…¶ä»–çš„è®¡ç®—å¦‚ä¸‹
$$
\begin{array}{l}{c_{1}=\frac{1}{N_{1}} \sum_{x_{i} \in R_{1}(j, s)} y_{i}=\frac{1}{2}(5.56+5.70)=5.63} \\ {c_{2}=\frac{1}{N_{2}} \sum_{x_{i} \in R_{2}(j, s)} y_{i}=\frac{1}{4}(5.91+6.40+6.90+7.95)=6.79} \\ {s_{m}=\min _{j, s}\left[\min _{c_{1} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]=2.294}\end{array}
$$
ç¬¬ä¸€æ¬¡åˆ‡åˆ†æ—¶ï¼Œå¯¹è±¡ä¸ºå…¨ä½“è¾“å…¥ï¼Œè®¡ç®—å‡ºæ¥çš„$s_m$å€¼å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

| åˆ‡åˆ†ç‚¹ | 1\.5     | 2\.5   | 3\.5        | 4\.5      | 5\.5     |
| ------ | -------- | ------ | ----------- | --------- | -------- |
| $s(m)$ | 3\.23468 | 2\.294 | 1\.31373333 | 0\.956725 | 1\.21752 |

å¯ä»¥çœ‹åˆ°ï¼Œå½“$s=4.5$æ—¶ï¼Œå–å¾—æœ€å°çš„$s(m)$å€¼ï¼Œæ­¤æ—¶çš„å›å½’ä¼°è®¡å€¼ä¸ºå…¨ä½“è¾“å…¥çš„å‡å€¼6.403ï¼Œé€’å½’æ±‚è§£å·¦å­æ ‘å’Œå³å­æ ‘çš„å›å½’ä¼°è®¡å€¼ï¼Œæœ€ç»ˆæ±‚è§£çš„å›å½’æ–¹ç¨‹ä¸º
$$
f(x)=\begin{cases}
5.723 & xâ‰¤3.5 \\ 
6.4 & 3.5<xâ‰¤4.5 \\ 
6.9 & 4.5<xâ‰¤5.5 \\ 
7.95 & x>5.5 
\end{cases}
$$


è¿™ä¸ªè¿‡ç¨‹å¯ä»¥ä½¿ç”¨`graphviz`æ¨¡å—æ˜¾ç¤ºå‡ºæ¥ã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9fe4w6aewj30yq0l6akv.jpg" alt="image-20191130004808546" style="zoom: 50%;" />

ä½¿ç”¨æœ¬æ–‡ä¸€å¼€å§‹æåˆ°çš„æ•°æ®ï¼Œå†³ç­–æ ‘å›å½’çš„ä»£ç å¦‚ä¸‹

```python
# å†³ç­–æ ‘å›å½’
from sklearn import tree
model_decisiontree_regression = tree.DecisionTreeRegress(min_weight_fraction_leaf=0.01)
plot_regression(model_decisiontree_regression, x_data, y_data)
```

![decision tree regression](https://tva1.sinaimg.cn/large/006tNbRwly1g9h99pbcjxj318g0m840d.jpg)

## 2.3 SVMå›å½’

å…ˆå›é¡¾ä¸€ä¸‹åœ¨åŸºæœ¬çº¿æ€§å¯åˆ†æƒ…å†µä¸‹çš„SVMæ¨¡å‹:
$$
\begin{array}{cl}{\min _{w, b, \xi}} & {\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ {\mathrm{s.t.}} & {-y_{i}\left(w \cdot x_{i}+b\right)-\xi_{i}+1 \leq 0, \quad i=1,2, \ldots, N} \\ {} & {-\xi_{i} \leq 0, \quad i=1,2, \ldots, N}\end{array}
$$
åˆ†ç±»SVMæ¨¡å‹ä¸­è¦è®©è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªæ ·æœ¬å°½å¯èƒ½è¿œç¦»è‡ªå·±ç±»åˆ«ä¸€ä¾§çš„æ”¯æŒå‘é‡ï¼Œå›å½’æ¨¡å‹ä¹Ÿä¸€æ ·ï¼Œæ²¿ç”¨çš„æ˜¯æœ€å¤§å»ºå“¥åˆ†ç±»å™¨çš„æ€æƒ³ã€‚

å¯¹äºå›å½’æ¨¡å‹ï¼Œä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°å’Œåˆ†ç±»æ¨¡å‹ä¿æŒä¸€è‡´ï¼Œä¾ç„¶æ˜¯$\min_{w,b}\frac{1}{2}\|w\|^{2}$ï¼Œä½†æ˜¯çº¦æŸæ¡ä»¶ä¸ä¸€æ ·ï¼Œå›å½’æ¨¡å‹çš„ç›®æ ‡æ˜¯è®©è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªæ ·æœ¬ç‚¹$(x_i,y_i)$å°½é‡æ‹Ÿåˆåˆ°ä¸€ä¸ªçº¿æ€§æ¨¡å‹$y_i=wx_i+b$ä¸Šï¼Œå¯¹äºä¸€èˆ¬çš„å›å½’æ¨¡å‹ä½¿ç”¨å‡æ–¹è¯¯å·®MSEä½œä¸ºæŸå¤±å‡½æ•°çš„ï¼Œä½†æ˜¯SVMå›å½’ä¸æ˜¯è¿™æ ·å®šä¹‰çš„ã€‚

SVMéœ€è¦æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå¸¸é‡$\varepsilon>0$ï¼Œå¯¹äºæŸä¸€ä¸ªç‚¹$(x_i,y_i)$ï¼Œå¦‚æœ$\left|y_i-wx_i-b\right|â‰¤\varepsilon$ï¼Œåˆ™å®Œå…¨æ²¡æœ‰æŸå¤±ï¼Œå¦‚æœ$\left|y_i-wx_i-b\right|>\varepsilon$ï¼Œåˆ™å¯¹åº”çš„æŸå¤±ä¸º$\left|y_i-wx_i-b\right|-\varepsilon$ï¼Œè¿™ä¸ªå’Œå‡æ–¹å·®æŸå¤±ä¸åŒï¼Œå¯¹äºå‡æ–¹å·®ï¼Œåªè¦$\left|y_i-wx_i-b\right|\neq 0$å°±ä¼šæœ‰æŸå¤±ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨è“è‰²æ¡å¸¦é‡Œé¢çš„ç‚¹æ˜¯æ²¡æœ‰æŸå¤±çš„ï¼Œä½†æ˜¯åœ¨å¤–é¢çš„ç‚¹æ˜¯æœ‰æŸå¤±çš„ï¼ŒæŸå¤±å¤§å°ä¸ºçº¢è‰²çº¿çš„é•¿åº¦ã€‚

![SVMå›å½’æŸå¤±å‡½æ•°](/Users/hz/Documents/codes/nlp/nlp-exercise/regression/pic/SVMå›å½’æŸå¤±å‡½æ•°.svg)

æ€»ç»“ä¸‹ï¼Œæˆ‘ä»¬çš„SVMå›å½’æ¨¡å‹çš„æŸå¤±å‡½æ•°åº¦é‡ä¸º
$$
\operatorname{err}\left(x_{i}, y_{i}\right)=\left\{\begin{array}{ll}{0} & {\left|y_{i}-w \cdot x_{i}-b\right| \leq \varepsilon} \\ {\left|y_{i}-w \cdot x_{i}-b\right|-\varepsilon} & {\left|y_{i}-w \cdot x_{i}-b\right|>\varepsilon}\end{array}\right.
$$
æœ‰äº†æŸå¤±å‡½æ•°ä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰SVMå›å½’çš„ç›®æ ‡å‡½æ•°ä¸º
$$
\begin{array}{l}{\min \frac{1}{2}\|w\|_{2}^{2}} \\ {\text { s.t }\left|y_{i}-w \cdot x_{i}-b\right| \leq \varepsilon(i=1,2, \ldots, m)}\end{array}
$$
è¿™ä¸ªæ¨¡å‹çš„æœ€ä¼˜è§£æ±‚è§£è¿‡ç¨‹è¿™é‡Œä¸å†èµ˜è¿°ï¼Œæœ‰å…´è¶£çš„å¯ä»¥çœ‹<a href="# å‚è€ƒ">å‚è€ƒ[2]æˆ–è€…[3]</a>ä¸­çš„è®ºè¿°ã€‚

ä½¿ç”¨æœ¬æ–‡ä¸€å¼€å§‹æåˆ°çš„æ•°æ®ï¼ŒSVMå›å½’çš„ä»£ç å¦‚ä¸‹

```python
# SVMå›å½’
from sklearn import svm
model_svr = svm.SVR()
plot_regression(model_svr, x_data, y_data)
```

![SVM regression](https://tva1.sinaimg.cn/large/006tNbRwly1g9hbc7cnklj318g0m840c.jpg)

# å‚è€ƒ

[1] æèˆª. ç»Ÿè®¡å­¦ä¹ æ–¹æ³•, æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾

[2] [CSDN-SVMå›å½’åšå®¢](https://blog.csdn.net/qq_32742009/article/details/81435141)

[3] [cnblog-SVMå›å½’åšå®¢](https://www.cnblogs.com/pinard/p/6113120.html)


