[toc]

# ä¸€ã€ä»€ä¹ˆæ˜¯ softmax å›å½’ï¼Ÿ

**softmax å›å½’**(softmax regression)å…¶å®æ˜¯ logistic å›å½’çš„ä¸€èˆ¬å½¢å¼ï¼Œlogistic å›å½’ç”¨äºäºŒåˆ†ç±»ï¼Œè€Œ softmax å›å½’ç”¨äºå¤šåˆ†ç±»ï¼Œå…³äº  logistic å›å½’å¯ä»¥çœ‹æˆ‘çš„è¿™ç¯‡åšå®¢ğŸ‘‰[[æœºå™¨å­¦ä¹ -logisticå›å½’åŸç†ä¸å®ç°]](https://zhuanlan.zhihu.com/p/95132284)ã€‚

å¯¹äºè¾“å…¥æ•°æ®$\{(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m)\}$æœ‰ $k$ ä¸ªç±»åˆ«ï¼Œå³$y_i \in \{1,2,\ldots,k\}$ï¼Œé‚£ä¹ˆ softmax å›å½’ä¸»è¦ä¼°ç®—è¾“å…¥æ•°æ® $x_i$ å½’å±äºæ¯ä¸€ç±»çš„æ¦‚ç‡ï¼Œå³
$$
h_{\theta}\left(x_i\right)=\left[\begin{array}{c}{p\left(y_i=1 | x_i ; \theta\right)} \\ {p\left(y_i=2 | x_i ; \theta\right)} \\ {\vdots} \\ {p\left(y_i=k | x_i ; \theta\right)}\end{array}\right]=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x_i}}\left[\begin{array}{c}{e^{\theta_{1}^{T} x_i}} \\ {e^{\theta_{2}^{T} x_i}} \\ {\vdots} \\ {e^{\theta_{k}^{T} x_i}}\end{array}\right]\tag{1}
$$
å…¶ä¸­ï¼Œ$\theta_1,\theta_2,\ldots,\theta_k \in \theta$æ˜¯æ¨¡å‹çš„å‚æ•°ï¼Œä¹˜ä»¥$\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x_i}}$æ˜¯ä¸ºäº†è®©æ¦‚ç‡ä½äº[0,1]å¹¶ä¸”æ¦‚ç‡ä¹‹å’Œä¸º 1ï¼Œsoftmax å›å½’å°†è¾“å…¥æ•°æ® $x_i$ å½’å±äºç±»åˆ« $j$ çš„æ¦‚ç‡ä¸º
$$
p\left(y_i=j | x_i ; \theta\right)=\frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\tag{2}
$$
ä¸Šé¢çš„å¼å­å¯ä»¥ç”¨ä¸‹å›¾å½¢è±¡åŒ–çš„è§£æ(æ¥è‡ªå°å¤§æå®æ¯…ã€Šä¸€å¤©ææ‡‚æ·±åº¦å­¦ä¹ ã€‹)ã€‚

![img](https://tva1.sinaimg.cn/large/006tNbRwly1g9yc7t1ye4j30jg0awq4j.jpg)

# äºŒã€åŸç†

## 2.1 æ¢¯åº¦ä¸‹é™æ³•å‚æ•°æ±‚è§£

softmax å›å½’çš„å‚æ•°çŸ©é˜µ $\theta$ å¯ä»¥è®°ä¸º
$$
\theta=\left[\begin{array}{c}{\theta_{1}^{T}} \\ {\theta_{2}^{T}} \\ {\vdots} \\ {\theta_{k}^{T}}\end{array}\right]\tag{3}
$$
å®šä¹‰ softmax å›å½’çš„ä»£ä»·å‡½æ•°
$$
L(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_i=j\right\} \log \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right]\tag{4}
$$
å…¶ä¸­ï¼Œ1{Â·}æ˜¯ç¤ºæ€§å‡½æ•°ï¼Œå³1{å€¼ä¸ºçœŸçš„è¡¨è¾¾å¼}=1ï¼Œ1{å€¼ä¸ºå‡çš„è¡¨è¾¾å¼}=0ã€‚è·Ÿ logistic å‡½æ•°ä¸€æ ·ï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•æœ€å°åŒ–ä»£ä»·å‡½æ•°ï¼Œä¸‹é¢æ±‚è§£ $\theta$ çš„æ¢¯åº¦ã€‚$L(\theta)$å…³äº $\theta_{j}$ çš„æ¢¯åº¦æ±‚è§£ä¸º
$$
\begin{aligned} 
\frac{\partial L(\theta)}{\partial \theta_{j}} 
&=-\frac{1}{m} \frac{\partial}{\partial \theta_{j}}\left[\sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_i=j\right\} \log \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right] \\ 
&=-\frac{1}{m} \frac{\partial}{\partial \theta_{j}}\left[\sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_i=j\right\}\left(\theta_{j}^{T} x_i-\log \sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}\right)\right] \\ 
&=-\frac{1}{m}\left[\sum_{i=1}^{m} 1\left\{y_i=j\right\}\left(x_i-\sum_{j=1}^{k} \frac{e^{\theta_{j}^{T} x_i} \cdot x_i}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i1\left\{y_i=j\right\}\left(1-\sum_{j=1}^{k} \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-\sum_{j=1}^{k} 1\left\{y_i=j\right\} \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}- \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-p\left(y_i=j | x_i ; \theta\right)\right)\right]
\end{aligned}\tag{5}
$$
æ„Ÿè°¢ CSDN åšä¸»[2]æä¾›äº†å¦å¤–ä¸€ç§æ±‚è§£æ–¹æ³•ï¼Œå…·ä½“å¦‚ä¸‹
$$
\begin{aligned} 
\frac{\partial L(\theta)}{\partial \theta_{j}} &=-\frac{1}{m}\left[\sum_{i=1}^{m} \frac{\partial}{\partial \theta_{j}}\left(1\left\{y_i=j\right\} \log \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}+\sum_{c \neq j}^{k} 1\left\{y_i=c\right\} \log \frac{e^{\theta_{c}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right]\\ 
&=-\frac{1}{m}\left[\sum_{i=1}^{m}\left(1\left\{y_i=j\right\}\left(x_i-\frac{e^{\theta_{j}^{T} x_i} \cdot x_i}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)+\sum_{c \neq j}^{k} 1\left\{y_i=c\right\}\left(-\frac{e^{\theta_{j}^{T} x_i} \cdot x_i}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right)\right]\\ 
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}\left(1-\frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)-\sum_{c \neq j}^{k} 1\left\{y_i=c\right\}\frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-1\left\{y_i=j\right\} p\left(y_i=j | x_i ; \theta\right)-\sum_{c \neq j}^{k} 1\left\{y_i=c\right\}p\left(y_i=j | x_i ; \theta\right)\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-\sum_{j=1}^{k} 1\left\{y_i=j\right\} p\left(y_i=j | x_i ; \theta\right)\right)\right] \\
&=-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-p\left(y_i=j | x_i ; \theta\right)\right)\right]
\end{aligned}\tag{6}
$$

## 2.2 æ¨¡å‹å‚æ•°ç‰¹ç‚¹

softmax å›å½’æœ‰ä¸€ä¸ªä¸å¯»å¸¸çš„ç‰¹ç‚¹ï¼šå®ƒæœ‰ä¸€ä¸ªâ€œå†—ä½™â€œçš„å‚æ•°é›†ã€‚ä¸ºäº†ä¾¿äºé˜è¿°è¿™ä¸€ç‰¹ç‚¹ï¼Œå‡è®¾æˆ‘ä»¬ä»å‚æ•°å‘é‡ $\theta_{j}$ ä¸­å‡å»å‘é‡ $\psi$ ï¼Œé‚£ä¹ˆå¯¹äºæ¦‚ç‡å‡½æ•°ï¼Œæˆ‘ä»¬æœ‰
$$
\begin{aligned} 
\left(y_i=j | x_i ; \theta\right)
&=\frac{e^{\left(\theta_{j}- \psi\right)^{T} x_i}}{\sum_{l=1}^{k} e^{\left(\theta_{l}- \psi\right)^{T} x_i}}\\
&=\frac{e^{\theta_{j}^{T} x_i }e^{-\psi^{T} x_i }}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}e^{-\psi^{T} x_i}}\\
&=\frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}
\end{aligned} \tag{7}
$$
æ¢å¥è¯è¯´ï¼Œä»å‚æ•°å‘é‡ä¸­çš„æ¯ä¸ªå…ƒç´  $\theta_j$ ä¸­å‡å» $\psi$ ä¸€ç‚¹ä¹Ÿä¸ä¼šå½±å“åˆ°å‡è®¾çš„ç±»åˆ«é¢„æµ‹ï¼è¿™è¡¨æ˜äº† softmax å›å½’çš„å‚æ•°ä¸­æ˜¯æœ‰å¤šä½™çš„ã€‚æ­£å¼åœ°è¯´ï¼Œ softmax æ¨¡å‹æ˜¯è¿‡å‚æ•°åŒ–çš„ï¼ˆ overparameterizedâ€‹ æˆ–å‚æ•°å†—ä½™çš„ï¼‰ï¼Œè¿™æ„å‘³ç€å¯¹ä»»ä½•ä¸€ä¸ªæ‹Ÿåˆæ•°æ®çš„å‡è®¾è€Œè¨€ï¼Œå¤šç§å‚æ•°å–å€¼æœ‰å¯èƒ½å¾—åˆ°åŒæ ·çš„å‡è®¾ $h_\theta$ï¼Œå³ä»è¾“å…¥ $x$ ç»è¿‡ä¸åŒçš„æ¨¡å‹å‚æ•°çš„å‡è®¾è®¡ç®—ä»è€Œå¾—åˆ°åŒæ ·çš„åˆ†ç±»é¢„æµ‹ç»“æœã€‚  

è¿›ä¸€æ­¥è¯´ï¼Œè‹¥ä»£ä»·å‡½æ•° $L(\theta)$ è¢«æŸç»„æ¨¡å‹å‚æ•° $(\theta_1, \theta_2,\ldots, \theta_k)$ æœ€å°åŒ–ï¼Œé‚£ä¹ˆå¯¹ä»»æ„çš„ $\psi$ ï¼Œä»£ä»·å‡½æ•°ä¹Ÿå¯ä»¥è¢« $(\theta_1 - \psi, \theta_2 - \psi,\ldots, \theta_k - \psi)$ æœ€å°åŒ–ã€‚å› æ­¤ï¼Œ $L(\theta)$ çš„æœ€å°å€¼æ—¶çš„å‚æ•°å¹¶ä¸å”¯ä¸€ã€‚ï¼ˆæœ‰è¶£çš„æ˜¯ï¼Œ $L(\theta)$ ä»æ˜¯å‡¸çš„ï¼Œå¹¶ä¸”åœ¨æ¢¯åº¦ä¸‹é™ä¸­ä¸ä¼šé‡åˆ°å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œä½†æ˜¯ Hessianâ€‹ çŸ©é˜µæ˜¯å¥‡å¼‚æˆ–ä¸å¯é€†çš„ï¼Œè¿™å°†ä¼šå¯¼è‡´åœ¨ç‰›é¡¿æ³•çš„ç›´æ¥å®ç°ä¸Šé‡åˆ°æ•°å€¼é—®é¢˜ã€‚ï¼‰  

æ³¨æ„åˆ°ï¼Œé€šè¿‡è®¾å®š $\psi = \theta_k$ ï¼Œæ€»æ˜¯å¯ä»¥ç”¨ $\theta_k - \psi = \vec{0}$ä»£æ›¿ $\theta_k$ ï¼Œè€Œä¸ä¼šå¯¹å‡è®¾å‡½æ•°æœ‰ä»»ä½•å½±å“ã€‚å› æ­¤ï¼Œå¯ä»¥å»æ‰å‚æ•°å‘é‡ $\theta$ ä¸­çš„æœ€åä¸€ä¸ªï¼ˆæˆ–è¯¥å‘é‡ä¸­ä»»æ„å…¶å®ƒä»»æ„ä¸€ä¸ªï¼‰å…ƒç´  $\theta_{k}$ ï¼Œè€Œä¸å½±å“å‡è®¾å‡½æ•°çš„è¡¨è¾¾èƒ½åŠ›ã€‚å®é™…ä¸Šï¼Œå› å‚æ•°å†—ä½™çš„ç‰¹æ€§ï¼Œä¸å…¶ä¼˜åŒ–å…¨éƒ¨çš„ $k\cdot n$ ä¸ªå‚æ•° $(\theta_1,\theta_2,\ldots,\theta_k)$ ï¼ˆå…¶ä¸­ $\theta_k \in \Re^{n}$ï¼‰ï¼Œä¹Ÿå¯ä»¤ $\theta_k = \vec{0}$ ï¼Œåªä¼˜åŒ–å‰©ä½™çš„ $(k-1) \cdot n$ ä¸ªå‚æ•°ï¼Œç®—æ³•ä¾ç„¶èƒ½å¤Ÿæ­£å¸¸å·¥ä½œã€‚

## 2.3 æ­£åˆ™åŒ–

å½“è®­ç»ƒæ•°æ®ä¸å¤Ÿå¤šçš„æ—¶å€™ï¼Œå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆç°è±¡ï¼Œæ‹Ÿåˆç³»æ•°å¾€å¾€éå¸¸å¤§ğŸ‘‰[[è¿‡æ‹ŸåˆåŸå› ](https://blog.csdn.net/u012328159/article/details/51089365)]ï¼Œä¸ºæ­¤åœ¨æŸå¤±å‡½æ•°åé¢åŠ ä¸Šä¸€ä¸ªæ­£åˆ™é¡¹ï¼Œå³
$$
L(\theta)=-\frac{1}{m}\left[\sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_i=j\right\} \log \frac{e^{\theta_{j}^{T} x_i}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_i}}\right]+\lambda\sum_{i=1}^{k} \sum_{j=1}^{n}\theta_{ij}^{2} \tag{8}
$$
é‚£ä¹ˆæ–°çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¸º
$$
\frac{\partial L(\theta)}{\partial \theta_{j}} =-\frac{1}{m}\left[\sum_{i=1}^{m} x_i\left(1\left\{y_i=j\right\}-p\left(y_i=j | x_i ; \theta\right)\right)\right]+\lambda\theta_j \tag{9}
$$
âš ï¸**æ³¨æ„**ï¼šä¸Šå¼ä¸­çš„ $\theta_j$ ä¸­çš„ $\theta_{j0}$ ä¸åº”è¯¥è¢«æƒ©ç½šï¼Œå› ä¸ºä»–æ˜¯ä¸€ä¸ªå¸¸æ•°é¡¹ï¼Œæ‰€ä»¥åœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™ä»…ä»…éœ€è¦å¯¹ $\theta_{j1},\theta_{j2},\dots,\theta_{jn}$ è¿›è¡Œæƒ©ç½šå³å¯ï¼Œè¿™ä¸ªä¼šåœ¨åé¢çš„ python ä»£ç ä¸­æåˆ°ğŸ˜ƒã€‚

## 2.4 softmax ä¸ logistic å›å½’çš„å…³ç³»

æ–‡ç« å¼€å¤´è¯´è¿‡ï¼Œsoftmax å›å½’æ˜¯ logistic å›å½’çš„ä¸€èˆ¬å½¢å¼ï¼Œlogistic å›å½’æ˜¯ softmax å›å½’åœ¨ $k=2$ æ—¶çš„ç‰¹æ®Šå½¢å¼ï¼Œä¸‹é¢é€šè¿‡å…¬å¼æ¨å¯¼æ¥çœ‹ä¸‹å½“ $k=2$ æ—¶ softmax å›å½’æ˜¯å¦‚ä½•é€€åŒ–æˆ logistic å›å½’ã€‚

å½“ $k=2$ æ—¶ï¼Œsoftmax å›å½’çš„å‡è®¾å‡½æ•°ä¸º
$$
h_{\theta}\left(x_i\right)=\left[\begin{array}{c}{p\left(y_i=1 | x_i ; \theta\right)} \\ {p\left(y_i=2 | x_i ; \theta\right)} \end{array}\right]=\frac{1}{e^{\theta_{1}^{T} x_i}+e^{\theta_{2}^{T} x_i}}\left[\begin{array}{c}{e^{\theta_{1}^{T} x_i}} \\ {e^{\theta_{2}^{T} x_i}} \end{array}\right]\tag{10}
$$
å‰é¢è¯´è¿‡ softmax å›å½’çš„å‚æ•°å…·æœ‰å†—ä½™æ€§ï¼Œä»å‚æ•°å‘é‡ $\theta_1,\theta_2$ ä¸­å‡å»å‘é‡ $\theta_1$å®Œå…¨ä¸å½±å“ç»“æœã€‚ç°åœ¨æˆ‘ä»¬ä»¤ $\theta'=\theta_2-\theta_1$ï¼Œå¹¶ä¸”ä¸¤ä¸ªå‚æ•°å‘é‡éƒ½å‡å» $\theta_1$ï¼Œåˆ™æœ‰
$$
\begin{aligned} 
h_{\theta}(x_i) &=\frac{1}{e^{\vec{0}^{T} x_i}+e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}\left[\begin{array}{c}{e^{\vec{0}^{T} x_i}} \\ {e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}\end{array}\right] \\ 
&=\left[\begin{array}{c}{\frac{1}{1+e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}} \\ {\frac{e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}{1+e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}}\end{array}\right] \\
&=\left[\begin{array}{c}{\frac{1}{1+e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}} \\ 1-{\frac{1}{1+e^{\left(\theta_{2}-\theta_{1}\right)^{T} x_i}}}\end{array}\right] \\
&=\left[\begin{array}{c}{\frac{1}{1+e^{\left(\theta'\right)^{T} x_i}}} \\ 1-{\frac{1}{1+e^{\left(\theta'\right)^{T} x_i}}}\end{array}\right] \\
\end{aligned}\tag{11}
$$
è¿™æ ·å°±åŒ–æˆäº† logistic å›å½’ã€‚

# ä¸‰ã€å®ç°

## 3.1 python æ‰‹åŠ¨å®ç°

è¿™é‡Œçš„æ•°æ®ä½¿ç”¨çš„æ˜¯ sklearn çš„ç®—æ³•åŒ…ç”Ÿæˆçš„éšæœºæ•°æ®ï¼Œå…¶ä¸­ï¼Œè®­ç»ƒæ•°æ®ä¸º 3750Ã—2 çš„æ•°æ®ï¼Œæµ‹è¯•æ•°æ®ä¸º 1250Ã—2 çš„æ•°æ®ï¼Œç”Ÿæˆä»£ç å¦‚ä¸‹

```python
def gen_dataset():
    from sklearn.datasets import load_iris
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import make_blobs
    import matplotlib.pyplot as plt
    np.random.seed(13)
    X, y = make_blobs(centers=4, n_samples = 5000)
    # ç»˜åˆ¶æ•°æ®åˆ†å¸ƒ
    plt.figure(figsize=(6,4))
    plt.scatter(X[:,0], X[:,1],c=y)
    plt.title("Dataset")
    plt.xlabel("First feature")
    plt.ylabel("Second feature")
    plt.show()

    # é‡å¡‘ç›®æ ‡ä»¥è·å¾—å…·æœ‰ (n_samples, 1)å½¢çŠ¶çš„åˆ—å‘é‡
    y = y.reshape((-1,1))
    # åˆ†å‰²æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    train_dataset = np.append(X_train,y_train, axis = 1)
    test_dataset = np.append(X_test,y_test, axis = 1)
    np.savetxt("train_dataset.txt", train_dataset, fmt="%.4f %.4f %d")
    np.savetxt("test_dataset.txt", test_dataset, fmt="%.4f %.4f %d")
```

æ•°æ®åˆ†å¸ƒæƒ…å†µå¦‚ä¸‹å›¾æ‰€ç¤º

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga0om326ljj30xc0m8q54.jpg" alt="dataset" style="zoom:50%;" />

softmax ç®—æ³•çš„æ ¸å¿ƒéƒ¨åˆ†å°±æ˜¯æ±‚è§£æ¢¯åº¦çŸ©é˜µï¼Œæˆ‘ä»¬è®¾è¾“å…¥æ•°æ®ä¸º $X=\{x_1,x_2,\ldots,x_m\}$ï¼Œè¿™æ˜¯ä¸€ä¸ª $mÃ—n$ çš„çŸ©é˜µï¼Œè¾“å‡ºç±»åˆ«ä¸º $y=\{y_1,y_2,\ldots,y_m\}$ï¼Œå…¶ä¸­ $y_i$ æ˜¯ä¸€ä¸ª $1Ã—k$ çš„one-hot çŸ©é˜µï¼Œ$k$ è¡¨ç¤ºç±»åˆ«ä¸ªæ•°ï¼Œé‚£ä¹ˆ $y$ å…¶å®æ˜¯ä¸€ä¸ª $mÃ—k$ çš„çŸ©é˜µï¼Œè¾“å…¥æ•°æ®å¯¹åº”çš„æ¦‚ç‡ä¸º $P=\{p_1,p_2,\ldots,p_m\}$ï¼Œ åŒæ ·çš„è¿™ä¹Ÿæ˜¯ä¸€ä¸ª $mÃ—k$ çš„çŸ©é˜µã€‚é‚£ä¹ˆæ ¹æ®å…¬å¼(9)ï¼Œå¯ä»¥çŸ¥é“ $\theta_j$ çš„æ¢¯åº¦ä¸º 
$$
\frac{\partial L(\theta)}{\partial \theta_{j}} =-\frac{1}{m}\left(y_i-P_i\right)^TX+\lambda\theta_j \tag{12}
$$
ç”±æ­¤å¯ä»¥æ¨å¯¼å‡º $\theta$ çš„å‚æ•°çŸ©é˜µä¸º
$$
\frac{\partial L(\theta)}{\partial \theta} =-\frac{1}{m}\left(y-P\right)^TX+\lambda\theta \tag{13}
$$
æ³¨æ„åˆ°è¿™é‡Œä¹Ÿè€ƒè™‘äº† $\theta_j$ çš„ç¬¬ 0 é¡¹ ï¼Œæ‰€ä»¥åœ¨å†™ä»£ç çš„æ—¶å€™éœ€è¦æŠŠ $\theta$ çš„ç¬¬ 0 åˆ—çš„æƒ©ç½šé¡¹å‡å»ã€‚

softmax å›å½’çš„ä»£ç å¦‚ä¸‹

```python
def load_dataset(file_path):
    dataMat = []
    labelMat = []
    fr = open(file_path)
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat, labelMat


def train(data_arr, label_arr, n_class, iters = 1000, alpha = 0.1, lam = 0.01):
    '''
    @description: softmax è®­ç»ƒå‡½æ•°
    @param {type} 
    @return: theta å‚æ•°
    '''    
    n_samples, n_features = data_arr.shape
    n_classes = n_class
    # éšæœºåˆå§‹åŒ–æƒé‡çŸ©é˜µ
    weights = np.random.rand(n_class, n_features)
    # å®šä¹‰æŸå¤±ç»“æœ
    all_loss = list()
    # è®¡ç®— one-hot çŸ©é˜µ
    y_one_hot = one_hot(label_arr, n_samples, n_classes)
    for i in range(iters):
        # è®¡ç®— m * k çš„åˆ†æ•°çŸ©é˜µ
        scores = np.dot(data_arr, weights.T)
        # è®¡ç®— softmax çš„å€¼
        probs = softmax(scores)
        # è®¡ç®—æŸå¤±å‡½æ•°å€¼
        loss = - (1.0 / n_samples) * np.sum(y_one_hot * np.log(probs))
        all_loss.append(loss)
        # æ±‚è§£æ¢¯åº¦
        dw = -(1.0 / n_samples) * np.dot((y_one_hot - probs).T, data_arr) + lam * weights
        dw[:,0] = dw[:,0] - lam * weights[:,0]
        # æ›´æ–°æƒé‡çŸ©é˜µ
        weights  = weights - alpha * dw
    return weights, all_loss
        

def softmax(scores):
    # è®¡ç®—æ€»å’Œ
    sum_exp = np.sum(np.exp(scores), axis = 1,keepdims = True)
    softmax = np.exp(scores) / sum_exp
    return softmax


def one_hot(label_arr, n_samples, n_classes):
    one_hot = np.zeros((n_samples, n_classes))
    one_hot[np.arange(n_samples), label_arr.T] = 1
    return one_hot


def predict(test_dataset, label_arr, weights):
    scores = np.dot(test_dataset, weights.T)
    probs = softmax(scores)
    return np.argmax(probs, axis=1).reshape((-1,1))


if __name__ == "__main__":
    #gen_dataset()
    data_arr, label_arr = load_dataset('train_dataset.txt')
    data_arr = np.array(data_arr)
    label_arr = np.array(label_arr).reshape((-1,1))
    weights, all_loss = train(data_arr, label_arr, n_class = 4)

    # è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡
    test_data_arr, test_label_arr = load_dataset('test_dataset.txt')
    test_data_arr = np.array(test_data_arr)
    test_label_arr = np.array(test_label_arr).reshape((-1,1))
    n_test_samples = test_data_arr.shape[0]
    y_predict = predict(test_data_arr, test_label_arr, weights)
    accuray = np.sum(y_predict == test_label_arr) / n_test_samples
    print(accuray)

    # ç»˜åˆ¶æŸå¤±å‡½æ•°
    fig = plt.figure(figsize=(8,5))
    plt.plot(np.arange(1000), all_loss)
    plt.title("Development of loss during training")
    plt.xlabel("Number of iterations")
    plt.ylabel("Loss")
    plt.show()
```

å‡½æ•°è¾“å‡ºçš„æµ‹è¯•æ•°æ®å‡†ç¡®ç‡ä¸º

```
0.9952
```

ç¨‹åºä¸­è®°å½•äº†æ¯ä¸ªå¾ªç¯çš„æŸå¤±å‡½æ•°ï¼Œå…¶å˜åŒ–æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga0p55zfokj318g0rsta0.jpg" alt="loss function" style="zoom:50%;" />

## 3.2 sklearn ç®—æ³•åŒ…å®ç°

`sklearn`çš„å®ç°æ¯”è¾ƒç®€å•ï¼Œä¸ logistic å›å½’çš„ä»£ç ç±»ä¼¼ã€‚

```python
def softmax_lib():
    data_arr, label_arr = load_dataset('train_dataset.txt')
    from sklearn import linear_model
    model_softmax_regression = linear_model.LogisticRegression(solver='lbfgs',multi_class="multinomial",max_iter=10)
    model_softmax_regression.fit(data_arr, label_arr)
    test_data_arr, test_label_arr = load_dataset('test_dataset.txt')
    y_predict = model_softmax_regression.predict(test_data_arr)
    accurcy = np.sum(y_predict == test_label_arr) / len(test_data_arr)
    print(accurcy)
```
è¾“å‡ºç»“æœä¸º
```
0.9848
```

æœ¬æ–‡çš„å®Œæ•´ä»£ç å’Œæ•°æ®å»ğŸ‘‰[[æˆ‘çš„ github]](https://github.com/HuStanding/nlp-exercise/blob/master/softmax/softmax.py)æŸ¥çœ‹

# å››ã€å‚è€ƒ

[1] https://zhuanlan.zhihu.com/p/34520042

[2] https://blog.csdn.net/u012328159/article/details/72155874

[3] https://zhuanlan.zhihu.com/p/56139075

[4] [https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0](https://zh.wikipedia.org/wiki/Softmaxå‡½æ•°)

[5] http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/

